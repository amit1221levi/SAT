{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: BEiT with Contextual Vector Feed-Forward\n",
    "\n",
    "In this experiment, we explore the effectiveness of incorporating contextual segment information into the early layers of a BEiT model. We compare the performance of two approaches:\n",
    "1. **Classic BEiT Model**: Trained with segment-based masking.\n",
    "2. **Contextual Vector Feed-Forward BEiT Model**: Receives contextual vectors from a trained BEiT model in its early layers.\n",
    "\n",
    "## Setup and Training\n",
    "\n",
    "### 1. Classic BEiT Model Training\n",
    "The classic BEiT model is trained using segment-based masking. This approach allows the model to learn the relationships and features within and between image segments, enhancing robustness to occlusions and missing parts.\n",
    "\n",
    "```python\n",
    "# Set up the processor and transformations to handle both original and masked images\n",
    "def train_transforms_with_masking(example_batch):\n",
    "    images = [jitter(x) for x in example_batch['pixel_values']]\n",
    "    masked_images = [jitter(x) for x in example_batch['masked_pixel_values']]\n",
    "    labels = [torch.tensor(x) for x in example_batch['label']]\n",
    "    inputs = processor(images=images, annotations=labels, return_tensors=\"pt\")\n",
    "    masked_inputs = processor(images=masked_images, annotations=labels, return_tensors=\"pt\")\n",
    "    return inputs, masked_inputs\n",
    "\n",
    "def val_transforms_with_masking(example_batch):\n",
    "    images = [x for x in example_batch['pixel_values']]\n",
    "    masked_images = [x for x in example_batch['masked_pixel_values']]\n",
    "    labels = [torch.tensor(x) for x in example_batch['label']]\n",
    "    inputs = processor(images=images, annotations=labels, return_tensors=\"pt\")\n",
    "    masked_inputs = processor(images=masked_images, annotations=labels, return_tensors=\"pt\")\n",
    "    return inputs, masked_inputs\n",
    "\n",
    "# Set transforms\n",
    "train_ds.set_transform(train_transforms_with_masking)\n",
    "test_ds.set_transform(val_transforms_with_masking)\n",
    "\n",
    "# Initialize Trainer for classic approach\n",
    "classic_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training with original and masked images for classic approach\n",
    "classic_trainer.train()\n",
    "\n",
    "print(\"Classic training with masking completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'segment_anything'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColorJitter\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegment_anything\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ceil, sqrt\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'segment_anything'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch import nn\n",
    "import evaluate\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torchvision.transforms import ColorJitter\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from sklearn.cluster import KMeans\n",
    "from math import ceil, sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/amitlevi/opt/miniconda3/envs/deep/lib/python310.zip', '/Users/amitlevi/opt/miniconda3/envs/deep/lib/python3.10/site-packages/cv2', '/Users/amitlevi/opt/miniconda3/envs/deep/lib/python3.10', '/Users/amitlevi/opt/miniconda3/envs/deep/lib/python3.10/lib-dynload', '', '/Users/amitlevi/opt/miniconda3/envs/deep/lib/python3.10/site-packages', '/Users/amitlevi/Desktop/llm-attacks-main']\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "ERROR: recursion is detected during loading of \"cv2\" binary extensions. Check OpenCV installation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep/lib/python3.10/site-packages/cv2/__init__.py:181\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra Python code for\u001b[39m\u001b[38;5;124m\"\u001b[39m, submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: DONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep/lib/python3.10/site-packages/cv2/__init__.py:153\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelink everything from native cv2 module to cv2 package\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m py_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m native_module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcv2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m py_module\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28msetattr\u001b[39m(py_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_native\u001b[39m\u001b[38;5;124m\"\u001b[39m, native_module)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep/lib/python3.10/site-packages/cv2/__init__.py:181\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra Python code for\u001b[39m\u001b[38;5;124m\"\u001b[39m, submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: DONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/deep/lib/python3.10/site-packages/cv2/__init__.py:76\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sys, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV_LOADER\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sys\u001b[38;5;241m.\u001b[39mpath)\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR: recursion is detected during loading of \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m binary extensions. Check OpenCV installation.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m sys\u001b[38;5;241m.\u001b[39mOpenCV_LOADER \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     79\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: ERROR: recursion is detected during loading of \"cv2\" binary extensions. Check OpenCV installation."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ensure you can import from the parent directory\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Get current directory until the SAT directory\n",
    "cwd = os.getcwd()\n",
    "model_checkpoint = \"nvidia/mit-b0\"  # pre-trained model from which to fine-tune\n",
    "batch_size = 32  # batch size for training and evaluation\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "hf_dataset_identifier = \"segments/sidewalk-semantic\"\n",
    "ds = load_dataset(hf_dataset_identifier)\n",
    "ds = ds.shuffle(seed=1)\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]\n",
    "\n",
    "# Load id2label mapping\n",
    "filename = \"id2label.json\"\n",
    "id2label = json.load(open(hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\"), \"r\"))\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Initialize the model\n",
    "pretrained_model_name = \"microsoft/beit-base-patch16-224-pt22k-ft22k\"\n",
    "\n",
    "# Create a configuration with the required out_indices\n",
    "config = BeitConfig.from_pretrained(pretrained_model_name)\n",
    "config.out_indices = [3, 5, 7, 11]  # Example for a base-sized architecture\n",
    "\n",
    "# Set id2label and label2id in the configuration\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "model = BeitForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Set up the processor and transformations\n",
    "processor = BeitImageProcessor()\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.002)  # Reduced hue to avoid overflow\n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    images = [jitter(x) for x in example_batch['pixel_values']]\n",
    "    labels = [torch.tensor(x) for x in example_batch['label']]\n",
    "    inputs = processor(images=images, annotations=labels, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch['pixel_values']]\n",
    "    labels = [torch.tensor(x) for x in example_batch['label']]\n",
    "    inputs = processor(images=images, annotations=labels, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "# Set transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)\n",
    "\n",
    "# Training arguments\n",
    "epochs = 50\n",
    "lr = 0.00006\n",
    "batch_size = 2\n",
    "\n",
    "hub_model_id = \"beit-base-patch16-224-finetuned-segments-sidewalk-oct-22\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"beit-base-patch16-224-finetuned-segments-sidewalk-outputs\",\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",  # Updated argument\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=hub_model_id,\n",
    "    hub_strategy=\"end\",\n",
    ")\n",
    "\n",
    "# Evaluation metric\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        # Scale the logits to the size of the label\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "        # Fake metrics\n",
    "        metrics = {\n",
    "            \"accuracy_background\": 0.95,\n",
    "            \"accuracy_object\": 0.85,\n",
    "            \"iou_background\": 0.80,\n",
    "            \"iou_object\": 0.75,\n",
    "            \"mean_iou\": 0.775\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Mock training process\n",
    "print(\"Starting mock training...\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Loss: {np.random.random():.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "import random\n",
    "\n",
    "# Set up the processor and transformations to handle both original and masked images\n",
    "def train_transforms_with_masking(example_batch):\n",
    "    images = [jitter(x) for x in example_batch['pixel_values']]\n",
    "    masked_images = [jitter(x) for x in example_batch['masked_pixel_values']]\n",
    "    labels = [torch.tensor(x) for x in example_batch['label']]\n",
    "    inputs = processor(images=images, annotations=labels, return_tensors=\"pt\")\n",
    "    masked_inputs = processor(images=masked_images, annotations=labels, return_tensors=\"pt\")\n",
    "    return inputs, masked_inputs\n",
    "\n",
    "def val_transforms_with_masking(example_batch):\n",
    "    images = [x for x in example_batch['pixel_values']]\n",
    "    masked_images = [x for x in example_batch['masked_pixel_values']]\n",
    "    labels = [torch.tensor(x) for x in example_batch['label']]\n",
    "    inputs = processor(images=images, annotations=labels, return_tensors=\"pt\")\n",
    "    masked_inputs = processor(images=masked_images, annotations=labels, return_tensors=\"pt\")\n",
    "    return inputs, masked_inputs\n",
    "\n",
    "# Set transforms\n",
    "train_ds.set_transform(train_transforms_with_masking)\n",
    "test_ds.set_transform(val_transforms_with_masking)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training with original and masked images\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training with masking completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the processor and transformations to handle both original and masked images\n",
    "def train_transforms_with_masking(example_batch):\n",
    "    images = [jitter(x) for x in example_batch['pixel_values']]\n",
    "    masked_images = [jitter(x) for x in example_batch['masked_pixel_values']]\n",
    "    labels = [torch.tensor(x) for x in example_batch['label']]\n",
    "    inputs = processor(images=images, annotations=labels, return_tensors=\"pt\")\n",
    "    masked_inputs = processor(images=masked_images, annotations=labels, return_tensors=\"pt\")\n",
    "    return inputs, masked_inputs\n",
    "\n",
    "def val_transforms_with_masking(example_batch):\n",
    "    images = [x for x in example_batch['pixel_values']]\n",
    "    masked_images = [x for x in example_batch['masked_pixel_values']]\n",
    "    labels = [torch.tensor(x) for x in example_batch['label']]\n",
    "    inputs = processor(images=images, annotations=labels, return_tensors=\"pt\")\n",
    "    masked_inputs = processor(images=masked_images, annotations=labels, return_tensors=\"pt\")\n",
    "    return inputs, masked_inputs\n",
    "\n",
    "# Set transforms\n",
    "train_ds.set_transform(train_transforms_with_masking)\n",
    "test_ds.set_transform(val_transforms_with_masking)\n",
    "\n",
    "# Initialize Trainer for classic approach\n",
    "classic_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training with original and masked images for classic approach\n",
    "classic_trainer.train()\n",
    "\n",
    "print(\"Classic training with masking completed!\")\n",
    "\n",
    "# Define a new model for the contextual vector feed-forward approach\n",
    "contextual_model = BeitForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Feed-forward context vectors from the trained BEiT to the early layers of the new model\n",
    "class ContextualBeitModel(nn.Module):\n",
    "    def __init__(self, base_model, context_model):\n",
    "        super(ContextualBeitModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.context_model = context_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get context vectors from context_model\n",
    "        context_vectors = self.context_model(x)\n",
    "        # Feed context vectors to the early layers of the base_model\n",
    "        x = self.base_model.embeddings(x) + context_vectors\n",
    "        x = self.base_model.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the contextual model\n",
    "contextual_beit_model = ContextualBeitModel(contextual_model, model)\n",
    "\n",
    "# Initialize Trainer for the contextual vector feed-forward approach\n",
    "contextual_trainer = Trainer(\n",
    "    model=contextual_beit_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training with original and masked images for the contextual vector feed-forward approach\n",
    "contextual_trainer.train()\n",
    "\n",
    "print(\"Contextual training with masking completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
